[SERVER]
11.11.11.11

[DOWNLOAD]
cp2k/8.2 https://github.com/cp2k/cp2k/releases/download/v8.2.0/cp2k-8.2.tar.bz2

[DEPENDENCY]
./jarvis -install kgcc/9.3.1 com
module purge
module use ./software/modulefiles
module load kgcc9/9.3.1
export CC=`which gcc`
export CXX=`which g++`
export FC=`which gfortran`
./jarvis -install openmpi/4.1.2 gcc
module load openmpi4/4.1.2
./jarvis -install gmp/6.2.0 gcc
./jarvis -install boost/1.72.0 gcc
./jarvis -install libint/2.6.0 gcc+mpi
./jarvis -install fftw/3.3.8 gcc+mpi
./jarvis -install openblas/0.3.18 gcc
module load openblas/0.3.18
./jarvis -install scalapack/2.1.0 gcc+mpi
./jarvis -install spglib/1.16.0 gcc
./jarvis -install libxc/5.1.4 gcc
./jarvis -install gsl/2.6 gcc
module load gsl/2.6
./jarvis -install plumed/2.6.2 gcc+mpi
./jarvis -install libvori/21.04.12 gcc
#release CP2K
tar -jxvf downloads/cp2k-8.2.tar.bz2

[ENV]
module purge
module load kgcc9/9.3.1
module load openmpi4/4.1.2
module load gsl/2.6

[APP]
app_name = CP2K
build_dir = ${JARVIS_ROOT}/cp2k-8.2/
binary_dir = ${JARVIS_ROOT}/cp2k-8.2/exe/local-cuda/
case_dir = ${JARVIS_ROOT}/cp2k-8.2/benchmarks/QS/

[BUILD]
make -j 128 ARCH=local-cuda VERSION=psmp

[CLEAN]
make -j 128 ARCH=local-cuda VERSION=psmp clean

[RUN]
run = numactl -C 0-63  mpirun --allow-run-as-root -x CUDA_VISIBLE_DEVICES=0,1 -np 64 -x OMP_NUM_THREADS=1
binary = cp2k.psmp H2O-256.inp
nodes = 1

[BATCH]
#!/bin/bash

logfile=cp2k.H2O-256.inp.log

nvidia-smi -pm 1
nvidia-smi -ac 1215,1410

echo 3 > /proc/sys/vm/drop_caches
echo "===run 32C*GPU===" >> $logfile
mpirun -np 32 -genv OMP_NUM_THREADS=1 -genv CUDA_VISIBLE_DEVICES=0 exe/local-cuda/cp2k.psmp benchmarks/QS/H2O-256.inp > cp2k.H2O-256.inp.log  >> $logfile 2>&1

echo 3 > /proc/sys/vm/drop_caches
echo "===run 32C*2GPU===" >> $logfile
mpirun -np 32 -genv OMP_NUM_THREADS=1 -genv CUDA_VISIBLE_DEVICES=0,1 exe/local-cuda/cp2k.psmp benchmarks/QS/H2O-256.inp > cp2k.H2O-256.inp.log  >> $logfile 2>&1


echo 3 > /proc/sys/vm/drop_caches
echo "===run 64C*GPU===" >> $logfile
mpirun -np 64 -genv OMP_NUM_THREADS=1 -genv CUDA_VISIBLE_DEVICES=0 exe/local-cuda/cp2k.psmp benchmarks/QS/H2O-256.inp > cp2k.H2O-256.inp.log  >> $logfile 2>&1

echo 3 > /proc/sys/vm/drop_caches
echo "===run 64C*2GPU===" >> $logfile
mpirun -np 32 -genv OMP_NUM_THREADS=1 -genv CUDA_VISIBLE_DEVICES=0,1 exe/local-cuda/cp2k.psmp benchmarks/QS/H2O-256.inp > cp2k.H2O-256.inp.log  >> $logfile 2>&1


echo 3 > /proc/sys/vm/drop_caches
echo "===run 128C*GPU===" >> $logfile
mpirun -np 128 -genv OMP_NUM_THREADS=1 -genv CUDA_VISIBLE_DEVICES=0 exe/local-cuda/cp2k.psmp benchmarks/QS/H2O-256.inp > cp2k.H2O-256.inp.log  >> $logfile 2>&1

echo 3 > /proc/sys/vm/drop_caches
echo "===run 128C*2GPU===" >> $logfile
mpirun -np 128 -genv OMP_NUM_THREADS=1 -genv CUDA_VISIBLE_DEVICES=0,1 exe/local-cuda/cp2k.psmp benchmarks/QS/H2O-256.inp > cp2k.H2O-256.inp.log  >> $logfile 2>&1







